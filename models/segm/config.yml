model:
  # deit
  deit_tiny_distilled_patch16_224:
    image_size: 224
    patch_size: 16
    d_model: 192
    n_heads: 3
    n_layers: 12
    normalization: deit
    distilled: true
  deit_small_distilled_patch16_224:
    image_size: 224
    patch_size: 16
    d_model: 384
    n_heads: 6
    n_layers: 12
    normalization: deit
    distilled: true
  deit_base_distilled_patch16_224:
    image_size: 224
    patch_size: 16
    d_model: 768
    n_heads: 12
    n_layers: 12
    normalization: deit
    distilled: true
  deit_base_distilled_patch16_384:
    image_size: 384
    patch_size: 16
    d_model: 768
    n_heads: 12
    n_layers: 12
    normalization: deit
    distilled: true
  # vit
  vit_base_patch8_384:
    image_size: 384
    patch_size: 8
    d_model: 768
    n_heads: 12
    n_layers: 12
    normalization: vit
    distilled: false
  vit_tiny_patch16_384:   # (use this for its finetuning on ImageNet-1k. Then interpolate for 256 x 256)
#    image_size: 384
    image_size: 256     # change for 256 x 256
    patch_size: 16
    d_model: 192
    n_heads: 3
    n_layers: 12
    normalization: vit
    distilled: false
    #
  swin_tiny_patch4_window7_224:
    image_size: 256
    patch_size: 4
    emb_dim: 96
    depths: [2,2,6,2]
    num_heads: [3, 6, 12, 24]
    window_size: 7
    mlp_ratio: 4
    normalization: swin
    patch_norm: true
    drop_path_rate: 0.2



  vit_tiny_patch16_224:   # new
    image_size: 224
    patch_size: 16
    d_model: 192
    n_heads: 3
    n_layers: 12
    normalization: vit
    distilled: false
  #
  vit_small_patch16_384:
    image_size: 256
    patch_size: 16
    d_model: 384
    n_heads: 6
    n_layers: 12
    normalization: vit
    distilled: false
  #
  vit_base_patch16_384:   # for seg-B-Mask/16
#    image_size: 384
    image_size: 256
    patch_size: 16
    d_model: 768
    n_heads: 12
    n_layers: 12
    normalization: vit
    distilled: false
  #
  vit_large_patch16_384:
    image_size: 384
    patch_size: 16
    d_model: 1024
    n_heads: 16
    n_layers: 24
    normalization: vit


  vit_small_patch32_384:
    image_size: 384
    patch_size: 32
    d_model: 384
    n_heads: 6
    n_layers: 12
    normalization: vit
    distilled: false
  vit_base_patch32_384:
    image_size: 384
    patch_size: 32
    d_model: 768
    n_heads: 12
    n_layers: 12
    normalization: vit
  vit_large_patch32_384:
    image_size: 384
    patch_size: 32
    d_model: 1024
    n_heads: 16
    n_layers: 24
    normalization: vit

decoder:
  linear: {}
  deeplab_dec:
    encoder_layer: -1
  mask_transformer:
    drop_path_rate: 0.0
    dropout: 0.1
    n_layers: 2

dataset:
  ade20k:
    epochs: 64
    eval_freq: 2
    batch_size: 8
    learning_rate: 0.001
    im_size: 512
    crop_size: 512
    window_size: 512
    window_stride: 512
  pascal_context:
    epochs: 256
    eval_freq: 8
    batch_size: 16
    learning_rate: 0.001
    im_size: 520
    crop_size: 480
    window_size: 480
    window_stride: 320
  cityscapes:
    epochs: 216
    eval_freq: 4
    batch_size: 8
    learning_rate: 0.01
    im_size: 1024
    crop_size: 768
    window_size: 768
    window_stride: 512
  coco:
    dataset_dir: 'E:/samsung/datasets/imagenetval/val5000'
    epochs: 256
#    batch_size: 16
    batch_size: 2
    learning_rate: 0.001
    im_size: 256
    crop_size: 256
    window_size: 256    # use in inference...
    window_stride: 256
  random:
    dataset_dir: 'E:/samsung/datasets/imagenetval/val5000'
    epochs: 256
    #    batch_size: 16
    batch_size: 2
    learning_rate: 0.001
    im_size: 256
    crop_size: 256
    window_size: 256    # use in inference...
    window_stride: 256
